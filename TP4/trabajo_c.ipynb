{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, silhouette_score\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>genres</th>\n",
       "      <th>imdb_id</th>\n",
       "      <th>original_title</th>\n",
       "      <th>overview</th>\n",
       "      <th>popularity</th>\n",
       "      <th>production_companies</th>\n",
       "      <th>production_countries</th>\n",
       "      <th>release_date</th>\n",
       "      <th>revenue</th>\n",
       "      <th>runtime</th>\n",
       "      <th>spoken_languages</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16000000.0</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>tt0114885</td>\n",
       "      <td>Waiting to Exhale</td>\n",
       "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
       "      <td>3.859495</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1995-12-22</td>\n",
       "      <td>81452156.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       budget  genres    imdb_id     original_title  \\\n",
       "2  16000000.0  Comedy  tt0114885  Waiting to Exhale   \n",
       "\n",
       "                                            overview  popularity  \\\n",
       "2  Cheated on, mistreated and stepped on, the wom...    3.859495   \n",
       "\n",
       "   production_companies  production_countries release_date     revenue  \\\n",
       "2                   1.0                   1.0   1995-12-22  81452156.0   \n",
       "\n",
       "   runtime  spoken_languages  vote_average  vote_count  \n",
       "2    127.0               1.0           6.1        34.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'movie_data.csv'\n",
    "data = pd.read_csv(path, sep=';', decimal='.')\n",
    "\n",
    "# Also make it datetime instead of a string and drop everything except action, comedy and drama in genre\n",
    "data['release_date'] = pd.to_datetime(data['release_date'])\n",
    "data = data[data['genres'].isin(['Action', 'Comedy', 'Drama'])]\n",
    "\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = 'genres'             # What to look for eventually\n",
    "\n",
    "numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns       # Numerical columns, only ones we need\n",
    "data = pd.DataFrame(data, columns=numerical_cols)                               # Make a new dataframe with only numerical columns\n",
    "\n",
    "# make an 20/80 split\n",
    "train_som, test_som = train_test_split(data, test_size=0.2, random_state=6)             # Split for later testing\n",
    "train_k, test_k = train_test_split(data, test_size=0.2, random_state=6)                 # Split for later testing\n",
    "train_group, test_group = train_test_split(data, test_size=0.2, random_state=6)         # Split for later testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from agrupacion import group\n",
    "from models import SOM\n",
    "from kmeanclass import KMeansClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the SOM on filtered data...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SOM with the filtered data's dimensions\n",
    "num_features_filtered = train_som.shape[1]\n",
    "som_filtered = SOM(height=10, width=10, input_dim=num_features_filtered)\n",
    "\n",
    "# Train the SOM\n",
    "print(\"Training the SOM on filtered data...\")\n",
    "som_filtered.train(train_som.to_numpy(), num_epochs=20, learning_rate=0.5, radius=3)\n",
    "\n",
    "print(f\"finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_means = KMeansClustering(k=3, max_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tp4\\\\europe.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/stig/Desktop/- TU:e/5Q1 ITBA Buenos Aires/72.75 Aprendizarje Automatico/72.75-ML/TP4/trabajo_c.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/stig/Desktop/-%20TU%3Ae/5Q1%20ITBA%20Buenos%20Aires/72.75%20Aprendizarje%20Automatico/72.75-ML/TP4/trabajo_c.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dataset \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stig/Desktop/-%20TU%3Ae/5Q1%20ITBA%20Buenos%20Aires/72.75%20Aprendizarje%20Automatico/72.75-ML/TP4/trabajo_c.ipynb#W2sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m countryIds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/stig/Desktop/-%20TU%3Ae/5Q1%20ITBA%20Buenos%20Aires/72.75%20Aprendizarje%20Automatico/72.75-ML/TP4/trabajo_c.ipynb#W2sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mtp4\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39meurope.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stig/Desktop/-%20TU%3Ae/5Q1%20ITBA%20Buenos%20Aires/72.75%20Aprendizarje%20Automatico/72.75-ML/TP4/trabajo_c.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     csv_reader \u001b[39m=\u001b[39m csv\u001b[39m.\u001b[39mreader(file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/stig/Desktop/-%20TU%3Ae/5Q1%20ITBA%20Buenos%20Aires/72.75%20Aprendizarje%20Automatico/72.75-ML/TP4/trabajo_c.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m csv_reader:\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/gpt/lib/python3.10/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'tp4\\\\europe.csv'"
     ]
    }
   ],
   "source": [
    "economicalSize = 3\n",
    "socialSize = 3\n",
    "geographicSize = 1\n",
    "\n",
    "grid_size = (5,5)\n",
    "learning_rate = 1\n",
    "epochs = 50*economicalSize\n",
    "\n",
    "dataset = []\n",
    "countryIds = []\n",
    "with open(\"tp4\\europe.csv\", 'r') as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        dataset.append(row[1:])\n",
    "        countryIds.append(row[0])\n",
    "countryIds = countryIds[1:]\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "geographicData = [[float(row[0])] for row in dataset[1:]]\n",
    "economicalData = [[row[1], row[2], row[4]] for row in dataset[1:]]\n",
    "socialData = [[row[3], row[5], row[6]] for row in dataset[1:]]\n",
    "\n",
    "\n",
    "# does scalling and normalization\n",
    "economicalData = sc.fit_transform(economicalData[1:]) \n",
    "socialData = sc.fit_transform(socialData[1:])   \n",
    "geographicData = sc.fit_transform(geographicData[1:])\n",
    "\n",
    "\"\"\" somEconomical = KohonenNetwork(economicalSize, grid_size, learning_rate, epochs)\n",
    "somEconomical.train(economicalData)\n",
    "#somEconomical.visualize(countryIds, \"Economical Features\")\n",
    "\n",
    "somSocial = KohonenNetwork(socialSize, grid_size, learning_rate, epochs)\n",
    "somSocial.train(socialData)\n",
    "#somSocial.visualize(countryIds, \"Social Features\")\n",
    "\n",
    "somgeographic = KohonenNetwork(geographicSize, grid_size, learning_rate, epochs)\n",
    "somgeographic.train(geographicData)\n",
    "#somgeographic.visualize(countryIds, \"Geographic Features\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# missing this \n",
    "# Realizar un gráfico que muestre las distancias promedio entre neuronas vecinas. \n",
    "# Analizar la cantidad de elementos que fueron asociados a cada neurona.\n",
    "\n",
    "\"\"\" avg_distances = somgeographic.avg_neighbor_distances()\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(avg_distances, annot=True, cmap='viridis')\n",
    "plt.title(\"Average Distances between Neighboring Neurons\")\n",
    "plt.show()\n",
    "\n",
    "neuron_count = somgeographic.neuron_counts()\n",
    "for neuron, count in neuron_count.items():\n",
    "    print(f\"Neuron {neuron} has {count} countries associated.\")\n",
    "\n",
    "somgeographic = KohonenNetwork(geographicSize, grid_size, learning_rate, epochs)\n",
    "somgeographic.train(geographicData)\n",
    "avg_distances = somgeographic.avg_neighbor_distances()\n",
    "somgeographic.visualize_grid(countryIds, avg_distances) \"\"\"\n",
    "\n",
    "# ... [Keep everything before this unchanged]\n",
    "\n",
    "# Combine the data\n",
    "combinedData = np.hstack([geographicData, economicalData, socialData])\n",
    "\n",
    "# Set learning rate\n",
    "learning_rate = 0.5\n",
    "\n",
    "# Different grid sizes for visualization\n",
    "grid_sizes = [(5,5)]\n",
    "\n",
    "for grid_size in grid_sizes:\n",
    "    combined_size = geographicSize + economicalSize + socialSize\n",
    "    epochs = 50 * combined_size\n",
    "\n",
    "    somCombined = KohonenNetwork(combined_size, grid_size, learning_rate, epochs)\n",
    "    somCombined.train(combinedData)\n",
    "\n",
    "    # Visualizing the SOM\n",
    "    avg_distances = somCombined.avg_neighbor_distances()\n",
    "    somCombined.visualize_grid(countryIds, avg_distances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
